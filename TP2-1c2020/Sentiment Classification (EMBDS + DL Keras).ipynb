{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [75.06 / 95.58] Organización de Datos\n",
    "## Trabajo Práctico 2: Competencia de Machine Learning\n",
    "### Grupo 18: DATAVID-20\n",
    "\n",
    "* 102732 - Bilbao, Manuel\n",
    "* 101933 - Karagoz, Filyan\n",
    "* 98684 - Markarian, Darío\n",
    "* 100901 - Stroia, Lautaro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clasificación de sentimientos se encarga de tomar una parte de un texto y decir si a la persona que lo redactó le gusto o no el tema de lo que está hablando. En este caso, lo podemos adaptar a nuestro problema de decidir si un tweet escrito por alguien describe un suceso real o falso.\n",
    "\n",
    "**Nos vamos a enfocar en:**\n",
    "*  Construir una Red Neuronal profunda para la clasificacion.\n",
    "*  Entrenar el modelo con Word Embeddings (usando Word2Vec).\n",
    "\n",
    "**El proceso es, mas o menos, el siguiente:**\n",
    "\n",
    "Tweets -> Embeddings -> Deep RRNN -> Red Fully connected -> Funcion de activacion (Sigmoidea en este caso) -> Target (1 o 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "import os\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = None #mostrar todas las filas del df\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar numeros de un texto\n",
    "def eliminar_numeros(text):\n",
    "    return re.sub(\"\\d+\", \"\",text)\n",
    "\n",
    "#Eliminar puntuacion\n",
    "def eliminar_puntuacion(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "#Pasar letras a minusculas\n",
    "def minusculas(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Eliminar caracteres especiales\n",
    "def eliminar_caracteres(text):\n",
    "    return re.sub('[^a-zA-Z0-9 \\n\\.]', '',text)\n",
    "\n",
    "#Eliminar urls\n",
    "def eliminar_url(text):\n",
    "    url_reg = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_reg.sub(r'',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up y split de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709,)\n",
      "(1904,)\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "train_set = pd.read_csv('train.csv')\n",
    "\n",
    "for data in [test_set,train_set]:\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_puntuacion(x))\n",
    "    data['text'] = data['text'].apply(lambda x: minusculas(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_numeros(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_caracteres(x))\n",
    "    data['text'] = data['text'].apply(lambda x: remove_stopwords(x))\n",
    "    data['text'] = data['text'].apply(lambda x: eliminar_url(x))  \n",
    "\n",
    "X = train_set['text'] #features\n",
    "y = train_set['target'] #variable a predecir\n",
    "\n",
    "#Me quedo con el 75% del set para entrenar, y el otro 25% para testear\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizacion de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "data_text = pd.concat([X,test_set.text])\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "max_len = max([len(text.split()) for text in data_text])\n",
    "\n",
    "#Cantidad de vocablos\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_tokens = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_tokens, maxlen=max_len, padding='post')\n",
    "X_valid_padded = pad_sequences(X_valid_tokens, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construyendo el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 21, 100)           2749800   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,762,697\n",
      "Trainable params: 2,762,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Defino el tamaño que van a tener los embeddings\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length = max_len))\n",
    "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "45/45 - 3s - loss: 0.6837 - accuracy: 0.5689 - val_loss: 0.6821 - val_accuracy: 0.5730\n",
      "Epoch 2/25\n",
      "45/45 - 2s - loss: 0.6804 - accuracy: 0.5695 - val_loss: 0.6611 - val_accuracy: 0.5746\n",
      "Epoch 3/25\n",
      "45/45 - 2s - loss: 0.4407 - accuracy: 0.7993 - val_loss: 0.4758 - val_accuracy: 0.7831\n",
      "Epoch 4/25\n",
      "45/45 - 2s - loss: 0.2141 - accuracy: 0.9245 - val_loss: 0.5687 - val_accuracy: 0.7789\n",
      "Epoch 5/25\n",
      "45/45 - 2s - loss: 0.1039 - accuracy: 0.9672 - val_loss: 0.6458 - val_accuracy: 0.7689\n",
      "Epoch 6/25\n",
      "45/45 - 2s - loss: 0.0669 - accuracy: 0.9809 - val_loss: 0.7889 - val_accuracy: 0.7363\n",
      "Epoch 7/25\n",
      "45/45 - 2s - loss: 0.0497 - accuracy: 0.9856 - val_loss: 0.7733 - val_accuracy: 0.7558\n",
      "Epoch 8/25\n",
      "45/45 - 2s - loss: 0.0349 - accuracy: 0.9918 - val_loss: 0.9650 - val_accuracy: 0.7511\n",
      "Epoch 9/25\n",
      "45/45 - 2s - loss: 0.0262 - accuracy: 0.9937 - val_loss: 0.8397 - val_accuracy: 0.7605\n",
      "Epoch 10/25\n",
      "45/45 - 2s - loss: 0.0308 - accuracy: 0.9926 - val_loss: 0.8498 - val_accuracy: 0.7574\n",
      "Epoch 11/25\n",
      "45/45 - 2s - loss: 0.0242 - accuracy: 0.9953 - val_loss: 0.8647 - val_accuracy: 0.7616\n",
      "Epoch 12/25\n",
      "45/45 - 2s - loss: 0.0240 - accuracy: 0.9946 - val_loss: 0.9537 - val_accuracy: 0.7421\n",
      "Epoch 13/25\n",
      "45/45 - 3s - loss: 0.0245 - accuracy: 0.9935 - val_loss: 0.9439 - val_accuracy: 0.7589\n",
      "Epoch 14/25\n",
      "45/45 - 2s - loss: 0.0204 - accuracy: 0.9954 - val_loss: 1.0061 - val_accuracy: 0.7264\n",
      "Epoch 15/25\n",
      "45/45 - 2s - loss: 0.0210 - accuracy: 0.9949 - val_loss: 0.9512 - val_accuracy: 0.7463\n",
      "Epoch 16/25\n",
      "45/45 - 2s - loss: 0.0188 - accuracy: 0.9949 - val_loss: 1.1162 - val_accuracy: 0.7295\n",
      "Epoch 17/25\n",
      "45/45 - 2s - loss: 0.0163 - accuracy: 0.9958 - val_loss: 1.0794 - val_accuracy: 0.7390\n",
      "Epoch 18/25\n",
      "45/45 - 2s - loss: 0.0189 - accuracy: 0.9956 - val_loss: 0.9094 - val_accuracy: 0.7658\n",
      "Epoch 19/25\n",
      "45/45 - 2s - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.9105 - val_accuracy: 0.7700\n",
      "Epoch 20/25\n",
      "45/45 - 2s - loss: 0.0140 - accuracy: 0.9963 - val_loss: 0.9873 - val_accuracy: 0.7516\n",
      "Epoch 21/25\n",
      "45/45 - 2s - loss: 0.0142 - accuracy: 0.9960 - val_loss: 1.0066 - val_accuracy: 0.7537\n",
      "Epoch 22/25\n",
      "45/45 - 2s - loss: 0.0137 - accuracy: 0.9953 - val_loss: 0.9117 - val_accuracy: 0.7463\n",
      "Epoch 23/25\n",
      "45/45 - 2s - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.9548 - val_accuracy: 0.7489\n",
      "Epoch 24/25\n",
      "45/45 - 2s - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.9145 - val_accuracy: 0.7505\n",
      "Epoch 25/25\n",
      "45/45 - 2s - loss: 0.0139 - accuracy: 0.9947 - val_loss: 1.0155 - val_accuracy: 0.7495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2ecbb0f60>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_padded,y_train, batch_size=128, epochs=25, validation_data=(X_valid_padded,y_valid),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predecimos con el set de test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = tokenizer.texts_to_sequences(test_set.text)\n",
    "test_tokens_padded = pad_sequences(test_tokens, maxlen=max_len)\n",
    "\n",
    "predictions = model.predict(test_tokens_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(test_set['id'],columns=['id'])\n",
    "predictions = model.predict(test_tokens_padded)\n",
    "submit['target'] = predictions\n",
    "submit['target'] = round(submit['target']).astype('int')\n",
    "#submit.to_csv('SUBMITS/embeddings+DL.csv', index=False)\n",
    "\n",
    "#Con este, obtuvimos 0.73735 en kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar la implementacion Word2Vec de Gensim. Primero vamos a tokenizar los tweets (preprocesados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27496\n"
     ]
    }
   ],
   "source": [
    "tokens_list = list()\n",
    "for tweet in data_text.values:\n",
    "    tokens_list.append(word_tokenize(tweet))\n",
    "\n",
    "model2 = gensim.models.Word2Vec(sentences=tokens_list,size=EMBEDDING_SIZE,min_count=1)\n",
    "\n",
    "#cantidad de vocablos aprendidos\n",
    "tokens_generated = list(model2.wv.vocab)\n",
    "print(len(tokens_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generacion de embeddings (usamos un set pre-entrenado de GloVe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = {}\n",
    "with open(os.environ['HOME']+'/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeds[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora, hay que convertir los embeddings en un vector de tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de tokens unicos:  27496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10876, 21)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(tokens_list)\n",
    "sequences = tokenizer2.texts_to_sequences(tokens_list)\n",
    "\n",
    "#longitud para armar los textos con un pad para que tengan la misma longitud\n",
    "word_index = tokenizer2.word_index\n",
    "print(\"Cantidad de tokens unicos: \",len(word_index))\n",
    "\n",
    "tokens_padded = pad_sequences(sequences, maxlen=max_len)\n",
    "tokens_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapear los embeddings del GloVe para cada palabra del vocabulario word_index y crear una matriz con esos vectores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_tokens = len(word_index) + 1\n",
    "embeddings = np.zeros((cant_tokens,EMBEDDING_SIZE))\n",
    "vec = []\n",
    "for word, i in word_index.items():\n",
    "    if i > cant_tokens:\n",
    "        continue\n",
    "    try:\n",
    "       embeddings[i] = embeds[word]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27497, 100)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generar modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 21, 100)           2749700   \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 32)                12864     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,762,597\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 2,749,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "embds_layer = Embedding(cant_tokens, EMBEDDING_SIZE, weights=[embeddings], input_length= max_len,\n",
    "              trainable=False)\n",
    "model2.add(embds_layer)\n",
    "model2.add(GRU(units=32,dropout=0.2, recurrent_dropout=0.2,return_sequences=False))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como concatene los 2 datasets (train y test) para entrenar todos los embeddings, voy a separarlos de nuevo, y al de train lo vuelvo a separar en un set de validacion y uno de train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = tokens_padded[0:train_set.shape[0]]\n",
    "new_test = tokens_padded[train_set.shape[0]:]\n",
    "\n",
    "X_train2,X_valid2,y_train2,y_valid2 = train_test_split(new_train,train_set.target, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "48/48 - 1s - loss: 0.6211 - accuracy: 0.6721 - val_loss: 0.5272 - val_accuracy: 0.7600\n",
      "Epoch 2/50\n",
      "48/48 - 1s - loss: 0.4889 - accuracy: 0.7807 - val_loss: 0.4525 - val_accuracy: 0.8020\n",
      "Epoch 3/50\n",
      "48/48 - 1s - loss: 0.4457 - accuracy: 0.8021 - val_loss: 0.4439 - val_accuracy: 0.8083\n",
      "Epoch 4/50\n",
      "48/48 - 1s - loss: 0.4375 - accuracy: 0.8066 - val_loss: 0.4410 - val_accuracy: 0.8072\n",
      "Epoch 5/50\n",
      "48/48 - 1s - loss: 0.4271 - accuracy: 0.8124 - val_loss: 0.4421 - val_accuracy: 0.8099\n",
      "Epoch 6/50\n",
      "48/48 - 1s - loss: 0.4197 - accuracy: 0.8142 - val_loss: 0.4379 - val_accuracy: 0.8041\n",
      "Epoch 7/50\n",
      "48/48 - 1s - loss: 0.4167 - accuracy: 0.8159 - val_loss: 0.4384 - val_accuracy: 0.8125\n",
      "Epoch 8/50\n",
      "48/48 - 1s - loss: 0.4143 - accuracy: 0.8191 - val_loss: 0.4359 - val_accuracy: 0.8093\n",
      "Epoch 9/50\n",
      "48/48 - 1s - loss: 0.4066 - accuracy: 0.8168 - val_loss: 0.4388 - val_accuracy: 0.8141\n",
      "Epoch 10/50\n",
      "48/48 - 1s - loss: 0.3999 - accuracy: 0.8240 - val_loss: 0.4411 - val_accuracy: 0.8136\n",
      "Epoch 11/50\n",
      "48/48 - 1s - loss: 0.3974 - accuracy: 0.8224 - val_loss: 0.4439 - val_accuracy: 0.8114\n",
      "Epoch 12/50\n",
      "48/48 - 1s - loss: 0.3937 - accuracy: 0.8238 - val_loss: 0.4388 - val_accuracy: 0.8162\n",
      "Epoch 13/50\n",
      "48/48 - 1s - loss: 0.3895 - accuracy: 0.8285 - val_loss: 0.4377 - val_accuracy: 0.8162\n",
      "Epoch 14/50\n",
      "48/48 - 1s - loss: 0.3852 - accuracy: 0.8301 - val_loss: 0.4371 - val_accuracy: 0.8146\n",
      "Epoch 15/50\n",
      "48/48 - 1s - loss: 0.3813 - accuracy: 0.8320 - val_loss: 0.4412 - val_accuracy: 0.8183\n",
      "Epoch 16/50\n",
      "48/48 - 1s - loss: 0.3738 - accuracy: 0.8390 - val_loss: 0.4425 - val_accuracy: 0.8114\n",
      "Epoch 17/50\n",
      "48/48 - 1s - loss: 0.3750 - accuracy: 0.8364 - val_loss: 0.4514 - val_accuracy: 0.8151\n",
      "Epoch 18/50\n",
      "48/48 - 1s - loss: 0.3708 - accuracy: 0.8382 - val_loss: 0.4381 - val_accuracy: 0.8114\n",
      "Epoch 19/50\n",
      "48/48 - 1s - loss: 0.3665 - accuracy: 0.8389 - val_loss: 0.4403 - val_accuracy: 0.8109\n",
      "Epoch 20/50\n",
      "48/48 - 1s - loss: 0.3647 - accuracy: 0.8432 - val_loss: 0.4436 - val_accuracy: 0.8109\n",
      "Epoch 21/50\n",
      "48/48 - 1s - loss: 0.3625 - accuracy: 0.8445 - val_loss: 0.4375 - val_accuracy: 0.8109\n",
      "Epoch 22/50\n",
      "48/48 - 1s - loss: 0.3503 - accuracy: 0.8495 - val_loss: 0.4499 - val_accuracy: 0.8114\n",
      "Epoch 23/50\n",
      "48/48 - 1s - loss: 0.3547 - accuracy: 0.8462 - val_loss: 0.4525 - val_accuracy: 0.8130\n",
      "Epoch 24/50\n",
      "48/48 - 1s - loss: 0.3451 - accuracy: 0.8508 - val_loss: 0.4476 - val_accuracy: 0.8078\n",
      "Epoch 25/50\n",
      "48/48 - 1s - loss: 0.3433 - accuracy: 0.8509 - val_loss: 0.4508 - val_accuracy: 0.8125\n",
      "Epoch 26/50\n",
      "48/48 - 1s - loss: 0.3418 - accuracy: 0.8485 - val_loss: 0.4526 - val_accuracy: 0.8104\n",
      "Epoch 27/50\n",
      "48/48 - 1s - loss: 0.3357 - accuracy: 0.8560 - val_loss: 0.4563 - val_accuracy: 0.8130\n",
      "Epoch 28/50\n",
      "48/48 - 1s - loss: 0.3322 - accuracy: 0.8583 - val_loss: 0.4566 - val_accuracy: 0.8088\n",
      "Epoch 29/50\n",
      "48/48 - 1s - loss: 0.3269 - accuracy: 0.8576 - val_loss: 0.4673 - val_accuracy: 0.7994\n",
      "Epoch 30/50\n",
      "48/48 - 1s - loss: 0.3219 - accuracy: 0.8623 - val_loss: 0.4641 - val_accuracy: 0.8114\n",
      "Epoch 31/50\n",
      "48/48 - 1s - loss: 0.3224 - accuracy: 0.8614 - val_loss: 0.4698 - val_accuracy: 0.8120\n",
      "Epoch 32/50\n",
      "48/48 - 1s - loss: 0.3208 - accuracy: 0.8623 - val_loss: 0.4706 - val_accuracy: 0.8030\n",
      "Epoch 33/50\n",
      "48/48 - 1s - loss: 0.3126 - accuracy: 0.8665 - val_loss: 0.4703 - val_accuracy: 0.8036\n",
      "Epoch 34/50\n",
      "48/48 - 1s - loss: 0.3128 - accuracy: 0.8695 - val_loss: 0.4764 - val_accuracy: 0.8072\n",
      "Epoch 35/50\n",
      "48/48 - 1s - loss: 0.3065 - accuracy: 0.8725 - val_loss: 0.4780 - val_accuracy: 0.8009\n",
      "Epoch 36/50\n",
      "48/48 - 1s - loss: 0.3024 - accuracy: 0.8716 - val_loss: 0.4718 - val_accuracy: 0.8062\n",
      "Epoch 37/50\n",
      "48/48 - 1s - loss: 0.2984 - accuracy: 0.8751 - val_loss: 0.4906 - val_accuracy: 0.8020\n",
      "Epoch 38/50\n",
      "48/48 - 1s - loss: 0.2961 - accuracy: 0.8718 - val_loss: 0.4830 - val_accuracy: 0.8030\n",
      "Epoch 39/50\n",
      "48/48 - 1s - loss: 0.2912 - accuracy: 0.8786 - val_loss: 0.4909 - val_accuracy: 0.8015\n",
      "Epoch 40/50\n",
      "48/48 - 1s - loss: 0.2987 - accuracy: 0.8762 - val_loss: 0.5042 - val_accuracy: 0.8009\n",
      "Epoch 41/50\n",
      "48/48 - 1s - loss: 0.2872 - accuracy: 0.8774 - val_loss: 0.4983 - val_accuracy: 0.8062\n",
      "Epoch 42/50\n",
      "48/48 - 1s - loss: 0.2886 - accuracy: 0.8779 - val_loss: 0.5012 - val_accuracy: 0.7994\n",
      "Epoch 43/50\n",
      "48/48 - 1s - loss: 0.2832 - accuracy: 0.8819 - val_loss: 0.5046 - val_accuracy: 0.8072\n",
      "Epoch 44/50\n",
      "48/48 - 1s - loss: 0.2832 - accuracy: 0.8793 - val_loss: 0.5051 - val_accuracy: 0.8041\n",
      "Epoch 45/50\n",
      "48/48 - 1s - loss: 0.2814 - accuracy: 0.8816 - val_loss: 0.5100 - val_accuracy: 0.8051\n",
      "Epoch 46/50\n",
      "48/48 - 1s - loss: 0.2730 - accuracy: 0.8846 - val_loss: 0.5036 - val_accuracy: 0.8093\n",
      "Epoch 47/50\n",
      "48/48 - 1s - loss: 0.2748 - accuracy: 0.8872 - val_loss: 0.4996 - val_accuracy: 0.8062\n",
      "Epoch 48/50\n",
      "48/48 - 1s - loss: 0.2758 - accuracy: 0.8833 - val_loss: 0.5128 - val_accuracy: 0.8036\n",
      "Epoch 49/50\n",
      "48/48 - 1s - loss: 0.2676 - accuracy: 0.8884 - val_loss: 0.5101 - val_accuracy: 0.8036\n",
      "Epoch 50/50\n",
      "48/48 - 1s - loss: 0.2690 - accuracy: 0.8865 - val_loss: 0.5178 - val_accuracy: 0.7973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1fc2c8860>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train2,y_train2, batch_size=120,epochs=50,validation_data=(X_valid2,y_valid2),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predecimos..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit2 = pd.DataFrame(test_set['id'],columns=['id'])\n",
    "predictions2 = model2.predict(new_test)\n",
    "submit2['target'] = predictions2\n",
    "submit2['target'] = round(submit2['target']).astype('int')\n",
    "#submit2.to_csv('SUBMITS/embeddings+DL-model2.csv', index=False)\n",
    "\n",
    "#Con este obtuvimos 0.80416 en KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
